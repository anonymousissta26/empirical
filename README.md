# What Humans Can Teach Symbolic Execution

What Humans Can Teach Symbolic Execution: New Research
Challenges Revealed by Human-Written Test Cases

## Data Availability
If you want to access data about the experiments of ParaSuit, you can download it at the following URL:
https://github.com/anonymousissta26/empirical/releases/tag/dataset

Download the following file from the URL
+ dataset.tar.xz

By clicking file or running the following codes on the terminal, you can download the data files.

```
$ wget https://github.com/anonymousissta26/empirical/releases/download/dataset/dataset.tar.xz
$ tar -xf dataset.tar.xz
```

You can access the test case directories for 6 programs: diff, find, gawk, gcal, grep, and sed.

Also, in each test directory, you will see the following files:
+ iteration-* : Iterations that used different option arguments and seed files.
   + info : This file expresses the KLEE command for the iteration.
   + test*.ktest : Each .ktest file is the generated test-cases for the target program. You can check the arguments of each testcase using the command as:
   ```
   ktest-tool <path to your testcase>
   ```
    the 'ktest-tool' is included in the KLEE-3.1. If you did not install KLEE-3.1, you can install it by following the instructions in [here](https://klee-se.org/releases/docs/v3.1/build-llvm13)


## Datset Structure
Here are brief descriptions of the dataset files. 
```
.
├── RQ1              <Setting the number and length of generated arguments similar to the human-generated ones>
    ├── featmaker    results in FeatMaker (FSE'24)
    ├── symtuner     results in SymTuner (ICSE'22)
    └── topseed      results in TopSeed (ICSE'25)
├── RQ2              <Injecting regular expressions to the test cases that are generated by state-of-the-arts>
    ├── featmaker    results in FeatMaker (FSE'24)
    ├── symtuner     results in SymTuner (ICSE'22)
    └── topseed      results in TopSeed (ICSE'25)
├── RQ3              <Guiding state-of-the-arts with efficient human-generated test cases>
    ├── _human_seed  efficient human seeds that are used to guide three state-of-the-art symbolic execution tools.
    ├── featmaker    results in FeatMaker (FSE'24)
    ├── symtuner     results in SymTuner (ICSE'22)
    └── topseed      results in TopSeed (ICSE'25)
└── vanilla          <Running state-of-the-arts without any modifications>
    ├── featmaker    results in FeatMaker (FSE'24)
    ├── symtuner     results in SymTuner (ICSE'22)
    └── topseed      results in TopSeed (ICSE'25)
    
```

## Reproducing Research Questions
### replay.py
```
$ python3 replay.py diff featmaker 1
usage: replay.py [program] [baseline] [rq_number]
```
| Option | Candidates | Description |
|:------:| :------: | :------------|
| `program`   | {diff, find, gawk, gcal, grep, sed} | Select the target program to reproduce|
| `baseline`  | {featmaker, symtuner, topseed} | Select the state-of-the-art technique used in the experiment |
| `rq_number` | {1, 2, 3} | Select the index of the research question you want to reproduce |


### Try reproducing with docker
We also present a docker container to try reproducing. First of all, clone our repository to your local directory.
```
$ git clone https://github.com/anonymousissta26/empirical.git
```

Next, build the docker container using the Dockerfile in the cloned directory.

```
/empirical$ docker build -t empirical-dataset .
```

Now, you can access to the docker container by using the following command.

```
/empirical$ docker run -it --name dataset empirical-dataset /bin/bash
```

Inside the docker container, you can reproduce the experimental results for each research question by running replay.py as mentioned above.
